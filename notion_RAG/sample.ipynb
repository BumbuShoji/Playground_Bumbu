{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import api key from env\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "env_path = os.path.join(os.path.dirname(\".env\"),\"/Users/shojibumbu/VScode_prac/\")\n",
    "load_dotenv(env_path)\n",
    "\n",
    "API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "integration_token = os.environ.get(\"NOTION_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import openai\n",
    "openai.api_key = API_KEY\n",
    "def generate_completion(role, prompt,direction):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4-turbo-2024-04-09\",\n",
    "        #model=\"gpt-3.5-turbo-1106\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": role},\n",
    "            {\"role\": \"user\", \"content\": f\"{prompt},{direction}\"}\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "        )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's the revised Python code for classifying the Fashion MNIST dataset using a multilayer perceptron implemented purely with NumPy. This code includes data normalization, one-hot encoding for training labels, model definition, training, and prediction phases.\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.utils import shuffle\n",
      "\n",
      "# Load the data\n",
      "x_train = np.load('drive/MyDrive/x_train.npy')\n",
      "t_train = np.load('drive/MyDrive/y_train.npy')\n",
      "x_test = np.load('drive/MyDrive/x_test.npy')\n",
      "\n",
      "# Normalize the data\n",
      "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
      "\n",
      "# Reshape the data\n",
      "x_train, x_test = x_train.reshape(x_train.shape[0], -1), x_test.reshape(x_test.shape[0], -1)\n",
      "\n",
      "# Convert labels to one-hot encoding\n",
      "t_train = np.eye(10)[t_train.astype(int)]\n",
      "\n",
      "# Split the data into training and validation\n",
      "x_train, x_val, t_train, t_val = train_test_split(x_train, t_train, test_size=10000, random_state=42)\n",
      "\n",
      "def softmax(x):\n",
      "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
      "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
      "\n",
      "def relu(x):\n",
      "    return np.maximum(0, x)\n",
      "\n",
      "def cross_entropy_loss(t, y):\n",
      "    return -np.sum(t * np.log(y + 1e-7)) / len(t)\n",
      "\n",
      "def create_batches(data, batch_size):\n",
      "    num_batches = len(data) // batch_size\n",
      "    batches = np.array_split(data, num_batches)\n",
      "    return batches\n",
      "\n",
      "class Dense:\n",
      "    def __init__(self, input_dim, output_dim):\n",
      "        self.W = np.random.randn(input_dim, output_dim) * np.sqrt(2. / input_dim)\n",
      "        self.b = np.zeros(output_dim)\n",
      "    \n",
      "    def forward(self, x):\n",
      "        self.x = x\n",
      "        return np.dot(x, self.W) + self.b\n",
      "    \n",
      "    def backward(self, dout):\n",
      "        self.dW = np.dot(self.x.T, dout)\n",
      "        self.db = np.sum(dout, axis=0)\n",
      "        return np.dot(dout, self.W.T)\n",
      "\n",
      "class Model:\n",
      "    def __init__(self):\n",
      "        self.layers = [\n",
      "            Dense(784, 128),\n",
      "            Dense(128, 64),\n",
      "            Dense(64, 10)\n",
      "        ]\n",
      "    \n",
      "    def forward(self, x):\n",
      "        for layer in self.layers[:-1]:\n",
      "            x = relu(layer.forward(x))\n",
      "        return softmax(self.layers[-1].forward(x))\n",
      "    \n",
      "    def backward(self, t, y):\n",
      "        delta = (y - t) / len(t)\n",
      "        for layer in reversed(self.layers):\n",
      "            delta = layer.backward(delta)\n",
      "    \n",
      "    def update_params(self, lr):\n",
      "        for layer in self.layers:\n",
      "            layer.W -= lr * layer.dW\n",
      "            layer.b -= lr * layer.db\n",
      "\n",
      "def train_model(model, x_train, t_train, x_val, t_val, lr, n_epochs, batch_size):\n",
      "    for epoch in range(n_epochs):\n",
      "        # Shuffle the training data\n",
      "        x_train, t_train = shuffle(x_train, t_train)\n",
      "        x_train_batches = create_batches(x_train, batch_size)\n",
      "        t_train_batches = create_batches(t_train, batch_size)\n",
      "        \n",
      "        for x_batch, t_batch in zip(x_train_batches, t_train_batches):\n",
      "            y_batch = model.forward(x_batch)\n",
      "            model.backward(t_batch, y_batch)\n",
      "            model.update_params(lr)\n",
      "        \n",
      "        # Validate the model\n",
      "        y_val = model.forward(x_val)\n",
      "        val_loss = cross_entropy_loss(t_val, y_val)\n",
      "        val_acc = accuracy_score(t_val.argmax(axis=1), y_val.argmax(axis=1))\n",
      "        \n",
      "        print(f'Epoch {epoch + 1}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}')\n",
      "\n",
      "# Initialize the model\n",
      "model = Model()\n",
      "\n",
      "# Train the model\n",
      "train_model(model, x_train, t_train, x_val, t_val, lr=0.01, n_epochs=100, batch_size=64)\n",
      "\n",
      "# Predict on test set\n",
      "t_pred = []\n",
      "for x in x_test:\n",
      "    x = x[np.newaxis, :]\n",
      "    y = model.forward(x)\n",
      "    pred = y.argmax(1).tolist()\n",
      "    t_pred.extend(pred)\n",
      "\n",
      "# Save predictions to CSV\n",
      "submission = pd.Series(t_pred, name='label')\n",
      "submission.to_csv('drive/MyDrive/submission_pred.csv', header=True, index_label='id')\n",
      "```\n",
      "\n",
      "This code defines a simple multilayer perceptron with two hidden layers and uses ReLU activation for hidden layers and softmax for the output layer. The network is trained using the cross-entropy loss function, and weights are updated using the backpropagation algorithm. The results are saved to a CSV file after making predictions on the test set.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"importosimportnumpyasnpimportpandasaspdfromsklearn.utilsimportshufflefromsklearn.metricsimportaccuracy_scorefromsklearn.model_selectionimporttrain_test_splitimportinspect#学習データx_train=np.load('drive/MyDrive/x_train.npy')t_train=np.load('drive/MyDrive/y_train.npy')#テストデータx_test=np.load('drive/MyDrive/x_test.npy')#データの前処理（正規化，one-hotencoding)x_train,x_test=x_train/255.,x_test/255.x_train,x_test=x_train.reshape(x_train.shape[0],-1),x_test.reshape(x_test.shape[0],-1)t_train=np.eye(N=10)[t_train.astype(int32).flatten()]#データの分割x_train,x_val,t_train,t_val=\\train_test_split(x_train,t_train,test_size=10000)defnp_log(x):returnnp.log(np.clip(x,1e-10,1e+10))defcreate_batch(data,batch_size)::paramdata:np.ndarray，入力データ:parambatch_size:int，バッチサイズnum_batches,mod=divmod(data.shape[0],batch_size)batched_data=np.split(data[:batch_size*num_batches],num_batches)ifmod:batched_data.append(data[batch_size*num_batches:])returnbatched_dataimportnumpyasnprng=np.random.RandomState(1234)random_state=42defrelu(x):returnnp.maximum(0,x)defderiv_relu(x):returnnp.where(x>0,1,0)defsoftmax(x):exp_x=np.exp(x-np.max(x,axis=-1,keepdims=True))returnexp_x/np.sum(exp_x,axis=-1,keepdims=True)defderiv_softmax(x):p=softmax(x)returnp*(1-p)defcrossentropy_loss(t,y):return-np.sum(t*np.log(y+1e-7))/len(t)classDense:def__init__(self,in_dim,out_dim):self.W=np.random.randn(in_dim,out_dim)*np.sqrt(2/in_dim)self.b=np.zeros(out_dim)defforward(self,x):self.x=xreturnnp.dot(x,self.W)+self.bdefbackward(self,delta):self.dW=np.dot(self.x.T,delta)self.db=np.sum(delta,axis=0)returnnp.dot(delta,self.W.T)classModel:def__init__(self,input_dim):self.layers=[Dense(input_dim,128),Dense(128,64),Dense(64,10)]defforward(self,x):forlayerinself.layers:x=relu(layer.forward(x))returnsoftmax(x)defbackward(self,t,y):delta=(y-t)/len(t)forlayerinreversed(self.layers):delta=layer.backward(delta)defupdate_params(self,lr):forlayerinself.layers:layer.W-=lr*layer.dWlayer.b-=lr*layer.dblr=0.01n_epochs=100batch_size=64mlp=Model(input_dim=28*28)fromsklearn.utilsimportshufflefromsklearn.metricsimportaccuracy_scoredeftrain_model(mlp,x_train,t_train,x_val,t_val,n_epochs=10):forepochinrange(n_epochs):losses_train=[]losses_valid=[]train_num=0train_true_num=0valid_num=0valid_true_num=0x_train,t_train=shuffle(x_train,t_train)x_train_batches,t_train_batches=create_batch(x_train,batch_size),create_batch(t_train,batch_size)x_val,t_val=shuffle(x_val,t_val)x_val_batches,t_val_batches=create_batch(x_val,batch_size),create_batch(t_val,batch_size)#モデルの訓練forx,tinzip(x_train_batches,t_train_batches):#順伝播y=mlp.forward(x)#損失の計算loss=crossentropy_loss(t,y)losses_train.append(loss)#パラメータの更新mlp.backward(t,y)mlp.update_params(lr)#精度を計算acc=accuracy_score(t.argmax(axis=1),y.argmax(axis=1))train_num+=x.shape[0]train_true_num+=acc*x.shape[0]#モデルの評価forx,tinzip(x_val_batches,t_val_batches):#順伝播y=mlp.forward(x)#損失の計算loss=crossentropy_loss(t,y)losses_valid.append(loss)acc=accuracy_score(t.argmax(axis=1),y.argmax(axis=1))valid_num+=x.shape[0]valid_true_num+=acc*x.shape[0]print('EPOCH:{},Train[Loss:{:.3f},Accuracy:{:.3f}],Valid[Loss:{:.3f},Accuracy:{:.3f}]'.format(epoch,np.mean(losses_train),train_true_num/train_num,np.mean(losses_valid),valid_true_num/valid_num))#train_model(mlp,x_train,t_train,x_val,t_val,n_epochs)t_pred=[]forxinx_test:#順伝播x=x[np.newaxis,:]y=mlp.forward(x)#修正:mlp(x)->mlp.forward(x)#モデルの出力を予測値のスカラーに変換pred=y.argmax(1).tolist()t_pred.extend(pred)submission=pd.Series(t_pred,name='label')submission.to_csv('drive/MyDrive/submission_pred.csv',header=True,index_label='id')\"\n",
    "\n",
    "direction = \"Please rewrite the code above.Let's classify a fashion version of MNIST (Fashion MNIST, 10 classes) using the multilayer perceptron.For more information on Fashion MNIST, please refer to the following linksFashion MNIST: https://github.com/zalandoresearch/fashion-mnistTarget ValueAccuracy 85RulesThe training data is given as x_train and t_train, and the test data as x_test.Prediction labels are not one_hot expressions, but class labels from 0 to 9.Do not use any training data other than the x_train and t_train specified in the bottom cell.The algorithm part of the multilayer perceptron should be implemented using NumPy only, referring to the exercises in Part 3. (Do not use sklearn, tensorflow, etc.).Translated with DeepL.com (free version)\"\n",
    "\n",
    "role = \"you are wise assistant\"\n",
    "\n",
    "response = generate_completion(role, prompt,direction)\n",
    "print(response.choices[0].message['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(id_='f530c7f9-d108-4428-8bbf-9cb85846da79', embedding=None, metadata={'page_id': '78db3e4c443e4ee0ba7b1464c44e7865'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='9502d0f5aa5abc413da085afa37bcadde061f3559803edc4e60cb4e97a1b04e6', text='\\t\\t庄司文武\\n\\t\\t所属\\u3000東京大学医学部健康総合科学科\\n\\t\\t誕生日\\u30002003/03/07\\n\\t\\t趣味\\u3000ダンス、コスプレ\\n\\t\\t連絡先情報\\n\\t\\tMail: bumbu.shoji@gmail.com\\n\\t\\tGitHub: \\n\\t\\thttps://github.com/BumbuShoji\\n\\n\\n自己紹介\\n初めまして。庄司文武と申します。\\n私は医療とAIの融合に関心があり、特にLLMの医療分野での応用について強く興味を持っています。私は医学部に所属し、医療からパブリックヘルスまで幅広く学んできました。そこで学んだのは医療はコミュニケーションがとても重要だということです。人間のコミュニケーションには言語が深く関わってきます。そのためLLMは医療において強いインパクトを持つと考えます。\\n医療分野でのAI、特にLLMの活用はまだ道半ばであり有効な活用法が見つけられているとは言えない状況です。それを見つけるためには医療とLLMと双方への理解を深めなければいけません。なので私はLLMへの理解をより深めるために、機械学習インターンや自主講座への参加などをしてきました。自主的な活動だけでなく学業においても、卒業論文や修士を通じて医療とAIの融合について扱っていこうと思っています。\\n医療は病気になった人を治すという個人的な問題である一方、全ての人が関わりうるものであり社会的な側面も持ちます。これからの社会にはAIがより深く関わってくることを考えると医療分野でのAIの活用は大きな利益を生み出すでしょうし、またとても興味深いテーマであります。私は研究や学習を続けながらこの分野に関わりたいと思っています。\\n\\n\\n職歴\\n株式会社Zeroboard\\n2023/09 ~ 現在\\n\\n機械学習エンジニアとしてインターンを行なっています。主な業務は自社サイト内でのRAGを用いたchatbotの改善やデータ分析、LLMについて最新情報の収集です。\\n\\n\\nスキル\\nバックエンド\\nPython\\n経験\\u3000一年半\\n主に機械学習、特にLLMの扱い方について学んできました。Hugging face, Llamaindexなどを使用した経験があります。\\nサーバ\\nAWS\\n経験\\u3000半年\\n現在行っているインターンの中で使用しており、sagemakerを中心とした基礎的な利用の経験があります。\\nその他\\nGit\\nDocker\\nSlack\\nNotion\\n\\n学歴\\n東京大学医学部健康総合科学科所属\\n2021/04\\u3000~\\u3000現在\\n現在大学４年\\n研究室\\n東京大学医学系研究科医療情報学教室所属\\n\\n医療のデータ基盤作成や医療分野での機械学習を行っている研究室に所属しております。\\n私はその中で医療分野でのLLMの応用について研究しています。具体的にはLLMにプロンプトを通じてロールを付与することで患者や被験者の振る舞いをどの程度再現できるかについて調べております。\\n\\n\\n言語\\n日本語 🇯🇵\\n母語\\n英語 🇺🇸\\n日常会話レベル\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]\n",
      "2003/03/07\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "from llama_index import ListIndex,NotionPageReader\n",
    "\n",
    "#Notionのシークレット\n",
    "page_ids = [\"78db3e4c443e4ee0ba7b1464c44e7865\"] #NotionのページID\n",
    "\n",
    "#読み込み\n",
    "documents = NotionPageReader(integration_token=integration_token).load_data(\n",
    "    page_ids=page_ids\n",
    ")\n",
    "print(documents)\n",
    "\n",
    "#indexDBに入れる\n",
    "index = ListIndex.from_documents(documents)\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"庄司の生年月日は？\")\n",
    "\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
