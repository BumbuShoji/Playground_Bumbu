{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import api key from env\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "env_path = os.path.join(os.path.dirname(\".env\"),\"/Users/shojibumbu/VScode_prac/\")\n",
    "load_dotenv(env_path)\n",
    "\n",
    "API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "integration_token = os.environ.get(\"NOTION_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import openai\n",
    "openai.api_key = API_KEY\n",
    "def generate_completion(role, prompt,direction):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4-turbo-2024-04-09\",\n",
    "        #model=\"gpt-3.5-turbo-1106\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": role},\n",
    "            {\"role\": \"user\", \"content\": f\"{prompt},{direction}\"}\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "        )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's the revised Python code for classifying the Fashion MNIST dataset using a multilayer perceptron implemented purely with NumPy. This code includes data normalization, one-hot encoding for training labels, model definition, training, and prediction phases.\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.utils import shuffle\n",
      "\n",
      "# Load the data\n",
      "x_train = np.load('drive/MyDrive/x_train.npy')\n",
      "t_train = np.load('drive/MyDrive/y_train.npy')\n",
      "x_test = np.load('drive/MyDrive/x_test.npy')\n",
      "\n",
      "# Normalize the data\n",
      "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
      "\n",
      "# Reshape the data\n",
      "x_train, x_test = x_train.reshape(x_train.shape[0], -1), x_test.reshape(x_test.shape[0], -1)\n",
      "\n",
      "# Convert labels to one-hot encoding\n",
      "t_train = np.eye(10)[t_train.astype(int)]\n",
      "\n",
      "# Split the data into training and validation\n",
      "x_train, x_val, t_train, t_val = train_test_split(x_train, t_train, test_size=10000, random_state=42)\n",
      "\n",
      "def softmax(x):\n",
      "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
      "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
      "\n",
      "def relu(x):\n",
      "    return np.maximum(0, x)\n",
      "\n",
      "def cross_entropy_loss(t, y):\n",
      "    return -np.sum(t * np.log(y + 1e-7)) / len(t)\n",
      "\n",
      "def create_batches(data, batch_size):\n",
      "    num_batches = len(data) // batch_size\n",
      "    batches = np.array_split(data, num_batches)\n",
      "    return batches\n",
      "\n",
      "class Dense:\n",
      "    def __init__(self, input_dim, output_dim):\n",
      "        self.W = np.random.randn(input_dim, output_dim) * np.sqrt(2. / input_dim)\n",
      "        self.b = np.zeros(output_dim)\n",
      "    \n",
      "    def forward(self, x):\n",
      "        self.x = x\n",
      "        return np.dot(x, self.W) + self.b\n",
      "    \n",
      "    def backward(self, dout):\n",
      "        self.dW = np.dot(self.x.T, dout)\n",
      "        self.db = np.sum(dout, axis=0)\n",
      "        return np.dot(dout, self.W.T)\n",
      "\n",
      "class Model:\n",
      "    def __init__(self):\n",
      "        self.layers = [\n",
      "            Dense(784, 128),\n",
      "            Dense(128, 64),\n",
      "            Dense(64, 10)\n",
      "        ]\n",
      "    \n",
      "    def forward(self, x):\n",
      "        for layer in self.layers[:-1]:\n",
      "            x = relu(layer.forward(x))\n",
      "        return softmax(self.layers[-1].forward(x))\n",
      "    \n",
      "    def backward(self, t, y):\n",
      "        delta = (y - t) / len(t)\n",
      "        for layer in reversed(self.layers):\n",
      "            delta = layer.backward(delta)\n",
      "    \n",
      "    def update_params(self, lr):\n",
      "        for layer in self.layers:\n",
      "            layer.W -= lr * layer.dW\n",
      "            layer.b -= lr * layer.db\n",
      "\n",
      "def train_model(model, x_train, t_train, x_val, t_val, lr, n_epochs, batch_size):\n",
      "    for epoch in range(n_epochs):\n",
      "        # Shuffle the training data\n",
      "        x_train, t_train = shuffle(x_train, t_train)\n",
      "        x_train_batches = create_batches(x_train, batch_size)\n",
      "        t_train_batches = create_batches(t_train, batch_size)\n",
      "        \n",
      "        for x_batch, t_batch in zip(x_train_batches, t_train_batches):\n",
      "            y_batch = model.forward(x_batch)\n",
      "            model.backward(t_batch, y_batch)\n",
      "            model.update_params(lr)\n",
      "        \n",
      "        # Validate the model\n",
      "        y_val = model.forward(x_val)\n",
      "        val_loss = cross_entropy_loss(t_val, y_val)\n",
      "        val_acc = accuracy_score(t_val.argmax(axis=1), y_val.argmax(axis=1))\n",
      "        \n",
      "        print(f'Epoch {epoch + 1}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}')\n",
      "\n",
      "# Initialize the model\n",
      "model = Model()\n",
      "\n",
      "# Train the model\n",
      "train_model(model, x_train, t_train, x_val, t_val, lr=0.01, n_epochs=100, batch_size=64)\n",
      "\n",
      "# Predict on test set\n",
      "t_pred = []\n",
      "for x in x_test:\n",
      "    x = x[np.newaxis, :]\n",
      "    y = model.forward(x)\n",
      "    pred = y.argmax(1).tolist()\n",
      "    t_pred.extend(pred)\n",
      "\n",
      "# Save predictions to CSV\n",
      "submission = pd.Series(t_pred, name='label')\n",
      "submission.to_csv('drive/MyDrive/submission_pred.csv', header=True, index_label='id')\n",
      "```\n",
      "\n",
      "This code defines a simple multilayer perceptron with two hidden layers and uses ReLU activation for hidden layers and softmax for the output layer. The network is trained using the cross-entropy loss function, and weights are updated using the backpropagation algorithm. The results are saved to a CSV file after making predictions on the test set.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"importosimportnumpyasnpimportpandasaspdfromsklearn.utilsimportshufflefromsklearn.metricsimportaccuracy_scorefromsklearn.model_selectionimporttrain_test_splitimportinspect#å­¦ç¿’ãƒ‡ãƒ¼ã‚¿x_train=np.load('drive/MyDrive/x_train.npy')t_train=np.load('drive/MyDrive/y_train.npy')#ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿x_test=np.load('drive/MyDrive/x_test.npy')#ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ï¼ˆæ­£è¦åŒ–ï¼Œone-hotencoding)x_train,x_test=x_train/255.,x_test/255.x_train,x_test=x_train.reshape(x_train.shape[0],-1),x_test.reshape(x_test.shape[0],-1)t_train=np.eye(N=10)[t_train.astype(int32).flatten()]#ãƒ‡ãƒ¼ã‚¿ã®åˆ†å‰²x_train,x_val,t_train,t_val=\\train_test_split(x_train,t_train,test_size=10000)defnp_log(x):returnnp.log(np.clip(x,1e-10,1e+10))defcreate_batch(data,batch_size)::paramdata:np.ndarrayï¼Œå…¥åŠ›ãƒ‡ãƒ¼ã‚¿:parambatch_size:intï¼Œãƒãƒƒãƒã‚µã‚¤ã‚ºnum_batches,mod=divmod(data.shape[0],batch_size)batched_data=np.split(data[:batch_size*num_batches],num_batches)ifmod:batched_data.append(data[batch_size*num_batches:])returnbatched_dataimportnumpyasnprng=np.random.RandomState(1234)random_state=42defrelu(x):returnnp.maximum(0,x)defderiv_relu(x):returnnp.where(x>0,1,0)defsoftmax(x):exp_x=np.exp(x-np.max(x,axis=-1,keepdims=True))returnexp_x/np.sum(exp_x,axis=-1,keepdims=True)defderiv_softmax(x):p=softmax(x)returnp*(1-p)defcrossentropy_loss(t,y):return-np.sum(t*np.log(y+1e-7))/len(t)classDense:def__init__(self,in_dim,out_dim):self.W=np.random.randn(in_dim,out_dim)*np.sqrt(2/in_dim)self.b=np.zeros(out_dim)defforward(self,x):self.x=xreturnnp.dot(x,self.W)+self.bdefbackward(self,delta):self.dW=np.dot(self.x.T,delta)self.db=np.sum(delta,axis=0)returnnp.dot(delta,self.W.T)classModel:def__init__(self,input_dim):self.layers=[Dense(input_dim,128),Dense(128,64),Dense(64,10)]defforward(self,x):forlayerinself.layers:x=relu(layer.forward(x))returnsoftmax(x)defbackward(self,t,y):delta=(y-t)/len(t)forlayerinreversed(self.layers):delta=layer.backward(delta)defupdate_params(self,lr):forlayerinself.layers:layer.W-=lr*layer.dWlayer.b-=lr*layer.dblr=0.01n_epochs=100batch_size=64mlp=Model(input_dim=28*28)fromsklearn.utilsimportshufflefromsklearn.metricsimportaccuracy_scoredeftrain_model(mlp,x_train,t_train,x_val,t_val,n_epochs=10):forepochinrange(n_epochs):losses_train=[]losses_valid=[]train_num=0train_true_num=0valid_num=0valid_true_num=0x_train,t_train=shuffle(x_train,t_train)x_train_batches,t_train_batches=create_batch(x_train,batch_size),create_batch(t_train,batch_size)x_val,t_val=shuffle(x_val,t_val)x_val_batches,t_val_batches=create_batch(x_val,batch_size),create_batch(t_val,batch_size)#ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´forx,tinzip(x_train_batches,t_train_batches):#é †ä¼æ’­y=mlp.forward(x)#æå¤±ã®è¨ˆç®—loss=crossentropy_loss(t,y)losses_train.append(loss)#ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ›´æ–°mlp.backward(t,y)mlp.update_params(lr)#ç²¾åº¦ã‚’è¨ˆç®—acc=accuracy_score(t.argmax(axis=1),y.argmax(axis=1))train_num+=x.shape[0]train_true_num+=acc*x.shape[0]#ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡forx,tinzip(x_val_batches,t_val_batches):#é †ä¼æ’­y=mlp.forward(x)#æå¤±ã®è¨ˆç®—loss=crossentropy_loss(t,y)losses_valid.append(loss)acc=accuracy_score(t.argmax(axis=1),y.argmax(axis=1))valid_num+=x.shape[0]valid_true_num+=acc*x.shape[0]print('EPOCH:{},Train[Loss:{:.3f},Accuracy:{:.3f}],Valid[Loss:{:.3f},Accuracy:{:.3f}]'.format(epoch,np.mean(losses_train),train_true_num/train_num,np.mean(losses_valid),valid_true_num/valid_num))#train_model(mlp,x_train,t_train,x_val,t_val,n_epochs)t_pred=[]forxinx_test:#é †ä¼æ’­x=x[np.newaxis,:]y=mlp.forward(x)#ä¿®æ­£:mlp(x)->mlp.forward(x)#ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ã‚’äºˆæ¸¬å€¤ã®ã‚¹ã‚«ãƒ©ãƒ¼ã«å¤‰æ›pred=y.argmax(1).tolist()t_pred.extend(pred)submission=pd.Series(t_pred,name='label')submission.to_csv('drive/MyDrive/submission_pred.csv',header=True,index_label='id')\"\n",
    "\n",
    "direction = \"Please rewrite the code above.Let's classify a fashion version of MNIST (Fashion MNIST, 10 classes) using the multilayer perceptron.For more information on Fashion MNIST, please refer to the following linksFashion MNIST: https://github.com/zalandoresearch/fashion-mnistTarget ValueAccuracy 85RulesThe training data is given as x_train and t_train, and the test data as x_test.Prediction labels are not one_hot expressions, but class labels from 0 to 9.Do not use any training data other than the x_train and t_train specified in the bottom cell.The algorithm part of the multilayer perceptron should be implemented using NumPy only, referring to the exercises in Part 3. (Do not use sklearn, tensorflow, etc.).Translated with DeepL.com (free version)\"\n",
    "\n",
    "role = \"you are wise assistant\"\n",
    "\n",
    "response = generate_completion(role, prompt,direction)\n",
    "print(response.choices[0].message['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(id_='f530c7f9-d108-4428-8bbf-9cb85846da79', embedding=None, metadata={'page_id': '78db3e4c443e4ee0ba7b1464c44e7865'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='9502d0f5aa5abc413da085afa37bcadde061f3559803edc4e60cb4e97a1b04e6', text='\\t\\tåº„å¸æ–‡æ­¦\\n\\t\\tæ‰€å±\\u3000æ±äº¬å¤§å­¦åŒ»å­¦éƒ¨å¥åº·ç·åˆç§‘å­¦ç§‘\\n\\t\\tèª•ç”Ÿæ—¥\\u30002003/03/07\\n\\t\\tè¶£å‘³\\u3000ãƒ€ãƒ³ã‚¹ã€ã‚³ã‚¹ãƒ—ãƒ¬\\n\\t\\té€£çµ¡å…ˆæƒ…å ±\\n\\t\\tMail: bumbu.shoji@gmail.com\\n\\t\\tGitHub: \\n\\t\\thttps://github.com/BumbuShoji\\n\\n\\nè‡ªå·±ç´¹ä»‹\\nåˆã‚ã¾ã—ã¦ã€‚åº„å¸æ–‡æ­¦ã¨ç”³ã—ã¾ã™ã€‚\\nç§ã¯åŒ»ç™‚ã¨AIã®èåˆã«é–¢å¿ƒãŒã‚ã‚Šã€ç‰¹ã«LLMã®åŒ»ç™‚åˆ†é‡ã§ã®å¿œç”¨ã«ã¤ã„ã¦å¼·ãèˆˆå‘³ã‚’æŒã£ã¦ã„ã¾ã™ã€‚ç§ã¯åŒ»å­¦éƒ¨ã«æ‰€å±ã—ã€åŒ»ç™‚ã‹ã‚‰ãƒ‘ãƒ–ãƒªãƒƒã‚¯ãƒ˜ãƒ«ã‚¹ã¾ã§å¹…åºƒãå­¦ã‚“ã§ãã¾ã—ãŸã€‚ãã“ã§å­¦ã‚“ã ã®ã¯åŒ»ç™‚ã¯ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãŒã¨ã¦ã‚‚é‡è¦ã ã¨ã„ã†ã“ã¨ã§ã™ã€‚äººé–“ã®ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã«ã¯è¨€èªãŒæ·±ãé–¢ã‚ã£ã¦ãã¾ã™ã€‚ãã®ãŸã‚LLMã¯åŒ»ç™‚ã«ãŠã„ã¦å¼·ã„ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆã‚’æŒã¤ã¨è€ƒãˆã¾ã™ã€‚\\nåŒ»ç™‚åˆ†é‡ã§ã®AIã€ç‰¹ã«LLMã®æ´»ç”¨ã¯ã¾ã é“åŠã°ã§ã‚ã‚Šæœ‰åŠ¹ãªæ´»ç”¨æ³•ãŒè¦‹ã¤ã‘ã‚‰ã‚Œã¦ã„ã‚‹ã¨ã¯è¨€ãˆãªã„çŠ¶æ³ã§ã™ã€‚ãã‚Œã‚’è¦‹ã¤ã‘ã‚‹ãŸã‚ã«ã¯åŒ»ç™‚ã¨LLMã¨åŒæ–¹ã¸ã®ç†è§£ã‚’æ·±ã‚ãªã‘ã‚Œã°ã„ã‘ã¾ã›ã‚“ã€‚ãªã®ã§ç§ã¯LLMã¸ã®ç†è§£ã‚’ã‚ˆã‚Šæ·±ã‚ã‚‹ãŸã‚ã«ã€æ©Ÿæ¢°å­¦ç¿’ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ³ã‚„è‡ªä¸»è¬›åº§ã¸ã®å‚åŠ ãªã©ã‚’ã—ã¦ãã¾ã—ãŸã€‚è‡ªä¸»çš„ãªæ´»å‹•ã ã‘ã§ãªãå­¦æ¥­ã«ãŠã„ã¦ã‚‚ã€å’æ¥­è«–æ–‡ã‚„ä¿®å£«ã‚’é€šã˜ã¦åŒ»ç™‚ã¨AIã®èåˆã«ã¤ã„ã¦æ‰±ã£ã¦ã„ã“ã†ã¨æ€ã£ã¦ã„ã¾ã™ã€‚\\nåŒ»ç™‚ã¯ç—…æ°—ã«ãªã£ãŸäººã‚’æ²»ã™ã¨ã„ã†å€‹äººçš„ãªå•é¡Œã§ã‚ã‚‹ä¸€æ–¹ã€å…¨ã¦ã®äººãŒé–¢ã‚ã‚Šã†ã‚‹ã‚‚ã®ã§ã‚ã‚Šç¤¾ä¼šçš„ãªå´é¢ã‚‚æŒã¡ã¾ã™ã€‚ã“ã‚Œã‹ã‚‰ã®ç¤¾ä¼šã«ã¯AIãŒã‚ˆã‚Šæ·±ãé–¢ã‚ã£ã¦ãã‚‹ã“ã¨ã‚’è€ƒãˆã‚‹ã¨åŒ»ç™‚åˆ†é‡ã§ã®AIã®æ´»ç”¨ã¯å¤§ããªåˆ©ç›Šã‚’ç”Ÿã¿å‡ºã™ã§ã—ã‚‡ã†ã—ã€ã¾ãŸã¨ã¦ã‚‚èˆˆå‘³æ·±ã„ãƒ†ãƒ¼ãƒã§ã‚ã‚Šã¾ã™ã€‚ç§ã¯ç ”ç©¶ã‚„å­¦ç¿’ã‚’ç¶šã‘ãªãŒã‚‰ã“ã®åˆ†é‡ã«é–¢ã‚ã‚ŠãŸã„ã¨æ€ã£ã¦ã„ã¾ã™ã€‚\\n\\n\\nè·æ­´\\næ ªå¼ä¼šç¤¾Zeroboard\\n2023/09 ~ ç¾åœ¨\\n\\næ©Ÿæ¢°å­¦ç¿’ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã¨ã—ã¦ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ³ã‚’è¡Œãªã£ã¦ã„ã¾ã™ã€‚ä¸»ãªæ¥­å‹™ã¯è‡ªç¤¾ã‚µã‚¤ãƒˆå†…ã§ã®RAGã‚’ç”¨ã„ãŸchatbotã®æ”¹å–„ã‚„ãƒ‡ãƒ¼ã‚¿åˆ†æã€LLMã«ã¤ã„ã¦æœ€æ–°æƒ…å ±ã®åé›†ã§ã™ã€‚\\n\\n\\nã‚¹ã‚­ãƒ«\\nãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰\\nPython\\nçµŒé¨“\\u3000ä¸€å¹´åŠ\\nä¸»ã«æ©Ÿæ¢°å­¦ç¿’ã€ç‰¹ã«LLMã®æ‰±ã„æ–¹ã«ã¤ã„ã¦å­¦ã‚“ã§ãã¾ã—ãŸã€‚Hugging face, Llamaindexãªã©ã‚’ä½¿ç”¨ã—ãŸçµŒé¨“ãŒã‚ã‚Šã¾ã™ã€‚\\nã‚µãƒ¼ãƒ\\nAWS\\nçµŒé¨“\\u3000åŠå¹´\\nç¾åœ¨è¡Œã£ã¦ã„ã‚‹ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ³ã®ä¸­ã§ä½¿ç”¨ã—ã¦ãŠã‚Šã€sagemakerã‚’ä¸­å¿ƒã¨ã—ãŸåŸºç¤çš„ãªåˆ©ç”¨ã®çµŒé¨“ãŒã‚ã‚Šã¾ã™ã€‚\\nãã®ä»–\\nGit\\nDocker\\nSlack\\nNotion\\n\\nå­¦æ­´\\næ±äº¬å¤§å­¦åŒ»å­¦éƒ¨å¥åº·ç·åˆç§‘å­¦ç§‘æ‰€å±\\n2021/04\\u3000~\\u3000ç¾åœ¨\\nç¾åœ¨å¤§å­¦ï¼”å¹´\\nç ”ç©¶å®¤\\næ±äº¬å¤§å­¦åŒ»å­¦ç³»ç ”ç©¶ç§‘åŒ»ç™‚æƒ…å ±å­¦æ•™å®¤æ‰€å±\\n\\nåŒ»ç™‚ã®ãƒ‡ãƒ¼ã‚¿åŸºç›¤ä½œæˆã‚„åŒ»ç™‚åˆ†é‡ã§ã®æ©Ÿæ¢°å­¦ç¿’ã‚’è¡Œã£ã¦ã„ã‚‹ç ”ç©¶å®¤ã«æ‰€å±ã—ã¦ãŠã‚Šã¾ã™ã€‚\\nç§ã¯ãã®ä¸­ã§åŒ»ç™‚åˆ†é‡ã§ã®LLMã®å¿œç”¨ã«ã¤ã„ã¦ç ”ç©¶ã—ã¦ã„ã¾ã™ã€‚å…·ä½“çš„ã«ã¯LLMã«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’é€šã˜ã¦ãƒ­ãƒ¼ãƒ«ã‚’ä»˜ä¸ã™ã‚‹ã“ã¨ã§æ‚£è€…ã‚„è¢«é¨“è€…ã®æŒ¯ã‚‹èˆã„ã‚’ã©ã®ç¨‹åº¦å†ç¾ã§ãã‚‹ã‹ã«ã¤ã„ã¦èª¿ã¹ã¦ãŠã‚Šã¾ã™ã€‚\\n\\n\\nè¨€èª\\næ—¥æœ¬èª ğŸ‡¯ğŸ‡µ\\næ¯èª\\nè‹±èª ğŸ‡ºğŸ‡¸\\næ—¥å¸¸ä¼šè©±ãƒ¬ãƒ™ãƒ«\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]\n",
      "2003/03/07\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "from llama_index import ListIndex,NotionPageReader\n",
    "\n",
    "#Notionã®ã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆ\n",
    "page_ids = [\"78db3e4c443e4ee0ba7b1464c44e7865\"] #Notionã®ãƒšãƒ¼ã‚¸ID\n",
    "\n",
    "#èª­ã¿è¾¼ã¿\n",
    "documents = NotionPageReader(integration_token=integration_token).load_data(\n",
    "    page_ids=page_ids\n",
    ")\n",
    "print(documents)\n",
    "\n",
    "#indexDBã«å…¥ã‚Œã‚‹\n",
    "index = ListIndex.from_documents(documents)\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"åº„å¸ã®ç”Ÿå¹´æœˆæ—¥ã¯ï¼Ÿ\")\n",
    "\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
